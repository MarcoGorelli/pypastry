#!/usr/bin/env python
import json
import pkgutil
import sys
from datetime import datetime
from importlib import import_module
from os import path
from uuid import uuid4

import pandas as pd
import tomlkit
from git import Repo
from sklearn.model_selection import cross_validate
from tomlkit.toml_document import TOMLDocument

sys.path.append('.')


def run(config: TOMLDocument):
    dataset = get_dataset(config)
    print(dataset)
    label_column = config['dataset']['label_column']
    X = dataset.drop(columns=[label_column])
    y = dataset[label_column]
    predictors = get_predictors()

    run_infos = evaluate_predictors(X, predictors, y)

    repo = Repo('.')
    repo.git.add(update=True)
    repo.index.commit('Run evaluation')
    sha = repo.head.commit.hexsha

    for i, run_info in enumerate(run_infos):
        run_id = sha + '.' + str(i)
        output_path = path.join('results', str(run_id)) + '.json'
        with open(output_path, 'w') as output_file:
            json.dump(run_info, output_file, indent=4)

        repo.index.add([output_path])
        print(run_info)
    repo.index.commit('Add results')


def evaluate_predictors(X, predictors, y):
    run_infos = []
    for predictor in predictors:
        start = datetime.utcnow()
        scores_dict = cross_validate(predictor, X, y)
        end = datetime.utcnow()

        scores = pd.DataFrame(scores_dict)
        mean_scores = scores.mean()
        sem_scores = scores.sem()
        results = dict(mean_scores.items())
        results.update({k + '_sem': v for k, v in sem_scores.items()})

        run_info = {
            'run_start': str(start),
            'run_end': str(end),
            'run_seconds': (end - start).total_seconds(),
            'results': results,
            'model_info': str(predictor),
        }
        run_infos.append(run_info)
    return run_infos


def get_dataset(config: TOMLDocument) -> pd.DataFrame:
    data_sources = {}
    for data_source in config['data_source']:
        url = data_source['url']
        data = pd.read_csv(url)
        data_sources[data_source['name']] = data
    return data_sources[config['dataset']['data_source']]


def get_predictors():
    predictors = []

    for _, module_name, _ in pkgutil.walk_packages(['.']):
        # print("Found submodule %s (is a package: %s)" % (modname, ispkg))
        module = import_module(module_name)
        try:
            predictor = module.get_predictor()
        except AttributeError:
            continue
        predictors.append(predictor)
    return predictors


if __name__ == "__main__":
    with open('config.toml') as toml_file:
        toml_config = tomlkit.loads(toml_file.read())
        run(toml_config)
